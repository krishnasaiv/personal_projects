{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeOL4V004yCP8aIEw/0GJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnasaiv/personal_projects/blob/main/searchEngine/1.1%20Text_Processing_Using_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJsjdECj_sfl"
      },
      "outputs": [],
      "source": [
        "#### Load all relevant Python libraries and a spaCy language model.\n",
        "\n",
        "import nltk\n",
        "import json \n",
        "import spacy \n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Open the provided JSON file. It contains a list of dictionaries with summaries from Wikipedia articles, where each dictionary has three key-value pairs. The keys title, text and url correspond to:\n",
        "\n",
        "# 1. Title of the Wikipedia article the text is taken from.\n",
        "# 2. Wikipedia article text. (In this dataset we included only the summary.)\n",
        "# 3. Link to the Wikipedia article.\n",
        "with open('data.json') as user_file:\n",
        "  file_contents = user_file.read()\n",
        "\n",
        "\n",
        "data = json.loads(file_contents)"
      ],
      "metadata": {
        "id": "l1h-W433AD_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create a Python function that takes in a text string, performs all operations described in the previous step, and outputs a list of tokens (lemmas).\n",
        "\n",
        "# Lowercases the text string.\n",
        "# Creates a spaCy document with the text lemmas and their attributes using a spaCy model of your choice.\n",
        "# Removes stop words, punctuation, and other unclassified lemmas.\n",
        "# Returns a list of tokens (lemmas) found in the text.\n",
        "\n",
        "def lemmatize_string(s):\n",
        "    s = s.lower()\n",
        "    doc = nlp(s)\n",
        "\n",
        "    tokens = [token.text for token in doc if not (\n",
        "        token.is_stop or \n",
        "        token.pos_ in ('PUNCT','SYM', 'X') or\n",
        "        token.text in '_\\n')]\n",
        "    \n",
        "    return list(set(tokens))\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "Nc2AS6NvDcrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this function to preprocess all text documents in the dataset (text field only), and add the resulting lists to the dictionaries from step 1. You should end up with a list of dictionaries, each of which now has four key-value pairs:\n",
        "\n",
        "# title: Title of the Wikipedia article the text is taken from.\n",
        "# text: Wikipedia article text. (In this dataset we included only the summary.)\n",
        "# tokenized_text: Tokenized Wikipedia article text.\n",
        "# url: Link to the Wikipedia article.\n",
        "\n",
        "\n",
        "for article in data:\n",
        "    article['tokenized_text'] = lemmatize_string(article['text'])"
      ],
      "metadata": {
        "id": "d0iwjbJzyp60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the new list of dictionaries in JSON format.\n",
        "\n",
        "with open('data_with_token.json', 'w') as fp:\n",
        "    json.dump(data, fp)"
      ],
      "metadata": {
        "id": "bia81ETx0E6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "KJy1sNKsyqpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# txt = \"POS Tagging in Spacy library is quite easy as seen in the below example. We just instantiate a Spacy object as doc. We iterate over doc object and use pos_ , tag_, to print the POS tag. Spacy also lets you access the detailed explanation of POS tags by using spacy.explain() function which is also printed in the same iteration along with POS tags.\"\n",
        "# #data[0]['text'].lower()\n",
        "# doc = nlp(txt)\n",
        "\n",
        "# # for t in s:\n",
        "# #     print(t, t.lemma_)\n",
        "\n",
        "# rows = []\n",
        "# rows.append([\"Word\", \"Position\", \"Lowercase\", \"Lemma\", \"POS\", \"Alphanumeric\",\"Stopword\"])\n",
        "\n",
        "# for token in doc:\n",
        "#     if not (token.is_stop or token.pos_ in ('PUNCT','SYM', 'X') or token.text == '\\n'):\n",
        "#         rows.append([token.text, str(token.i), token.lower_,    token.lemma_, token.pos_, str(token.is_alpha),    str(token.is_stop)])\n",
        "# columns = zip(*rows)\n",
        "# column_widths = [max(len(item) for item in col) for col in columns]\n",
        "\n",
        "\n",
        "# for row in rows:\n",
        "#     print(''.join(' {:{width}} '.format(row[i], width=column_widths[i])  for i in range(0, len(row))))"
      ],
      "metadata": {
        "id": "T_jFSYZNGgMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(rows)"
      ],
      "metadata": {
        "id": "plqZ6GIXvdUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}