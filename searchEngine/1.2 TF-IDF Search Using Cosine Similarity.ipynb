{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcEaQSO/AUr4uXftX7pN/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnasaiv/personal_projects/blob/main/searchEngine/1.2%20TF-IDF%20Search%20Using%20Cosine%20Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dependencies, or libraries required to complete the step."
      ],
      "metadata": {
        "id": "oR78xyPm1i7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "qHgrp_ZJ1LEO"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "import spacy \n",
        "import math\n",
        "from itertools import chain\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open the JSON file containing the provided dataset\n",
        "which is a list of dictionaries containing a title, URL, and summary from a set of Wikipedia articles.\n"
      ],
      "metadata": {
        "id": "fy9fwFYR1ny5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data_lemmatized.json') as fp:\n",
        "  file_contents = fp.read()\n",
        "\n",
        "data = json.loads(file_contents)\n",
        "# data"
      ],
      "metadata": {
        "id": "4xCwyEXX1sAw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a vocabulary for your dataset, for example, a set of tokens occurring in all your texts"
      ],
      "metadata": {
        "id": "t5WbeOSxISIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs = [i['tokenized_text'] for i in data]\n",
        "vocab = list(set(chain.from_iterable(tokenized_docs)))\n",
        "\n",
        "with open('data_vocab.json', 'w') as fp:\n",
        "    json.dump(vocab, fp)"
      ],
      "metadata": {
        "id": "v4smxlH1IUT-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count how many times each unique token appears in the corpus, you will need these counts for the next step."
      ],
      "metadata": {
        "id": "hiTI_KneXTtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### For each doc Create (a dictionaries with) token frequencies (using 'Counter') \n",
        "docs_token_counter = dict()\n",
        "for doc in data:\n",
        "    docs_token_counter[doc['title']] = dict(Counter(doc['tokenized_text']))\n",
        "\n",
        "### For each token in corpus vocabulary, count in how many documents it occurs\n",
        "num_docs_with_token = dict()\n",
        "for token in vocab:\n",
        "    num_docs_with_token[token] = sum([ 1 if token in doc['tokenized_text'] else 0 for doc in data]) \n"
      ],
      "metadata": {
        "id": "gcYVL-YBWjBT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Develop a Tf-Idf-builder function that accepts a vocabulary and a text string and returns the document’s Tf-Idf vector.\n"
      ],
      "metadata": {
        "id": "nzHT-IaPIU4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TfIdf(tokenized_input_string, vocab = vocab, total_docs = len(data), num_docs_with_token = num_docs_with_token ):\n",
        "    #### Create a zero vector from corpus with sorted keys. \n",
        "    #### This is to ensure the value for each element of the vector to represent the same word in each document’s vector\n",
        "    vec = OrderedDict( ((i,0) for i in sorted(vocab)) )\n",
        "    input_token_frequencies = Counter(tokenized_input_string)\n",
        "\n",
        "    for token in tokenized_input_string:\n",
        "        if token in vec:\n",
        "            ######## TF\n",
        "            tf = input_token_frequencies[token] / len(tokenized_input_string)\n",
        "\n",
        "            ######## IDF\n",
        "            docs_with_key = num_docs_with_token[token]\n",
        "            idf = 0 if docs_with_key==0 else total_docs/ docs_with_key \n",
        "\n",
        "            ######## TF-IDF\n",
        "            vec[token] = tf * idf\n",
        "    \n",
        "    return list(vec.values())"
      ],
      "metadata": {
        "id": "Ja7z5JdNIXSV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Run the Tf-Idf-builder function on all text documents in the dataset.\n",
        "\n",
        "### 2. Update the original list of dictionaries by adding tf_idf as a new field to each document dictionary called tf_idf"
      ],
      "metadata": {
        "id": "gEyn5inSIY1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in data:\n",
        "    doc['tf_idf'] = TfIdf(doc['tokenized_text'])\n",
        "\n",
        "with open('data_tfidf.json', 'w') as fp:\n",
        "    json.dump(data, fp)"
      ],
      "metadata": {
        "id": "7bZ8pRjZIbDj"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a search function to compute cosine similarities between the document Tf-Idf vectors and the query Tf-Idf vector."
      ],
      "metadata": {
        "id": "3gLCG-V7IfO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a,b):\n",
        "    dot_product = 0\n",
        "    for i in range(len(a)):\n",
        "        dot_product += a[i] * b[i]\n",
        "\n",
        "    mag_1 = math.sqrt(sum([x**2 for x in a]))\n",
        "    mag_2 = math.sqrt(sum([x**2 for x in b]))\n",
        "\n",
        "    return round(dot_product / (mag_1 * mag_2), 4)\n",
        "\n",
        "def tokenizer(input_string):\n",
        "    input_string = input_string.lower()\n",
        "    doc = nlp(input_string.replace(\"\\n\", \"\"))\n",
        "    tokens = [token.lemma_ for token in doc if \n",
        "              not (\n",
        "                    token.is_stop or                    # Remove Stop Words\n",
        "                    token.is_punct or                   # Remove punctutaion\n",
        "                    token.pos_ in ('SYM', 'X') or       # Remove symbols & unclassified POS\n",
        "                    token.dep_ == \"\" or                 # Remove unclassified dependencies\n",
        "                    token.like_num or                   # Remove numeric or numeric like tokens\n",
        "                    token.text in '_\\n '                # Remove misc characters\n",
        "                   )\n",
        "              ]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def search(query, data = data): # the input 'data' is the master dictionary having title, text, tokens, url & tfidf score\n",
        "    tokens = tokenizer(query)\n",
        "    query_TfIdf =  TfIdf(tokens)\n",
        "\n",
        "    if sum(query_TfIdf) == 0:                   # if sum of query tfidf is zero, this means none of the words in the query exist in our vocabulary and hence relevance with any of the docs will be zero\n",
        "        return \"No Relevant documents Found\"    # not catching this here will throw a division by zero error in cosine similarity function when divided by it's magnitude ( which is 0)\n",
        "\n",
        "    search_relevance_ranks = [ {'title': doc['title'], 'rank':cosine_similarity(query_TfIdf, doc['tf_idf'])}  for doc in data ]\n",
        "    search_relevance_ranks = sorted(\n",
        "                                    [i for i in search_relevance_ranks if i['rank'] > 0]\n",
        "                                    , key = lambda x: x['rank']\n",
        "                                    , reverse = True\n",
        "                                    )\n",
        "\n",
        "    return search_relevance_ranks #sorted(search_relevance_ranks, key= lambda x: list(x.values())[0], reverse=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0j-0gfZCIhYk"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search(\"covid-19\")"
      ],
      "metadata": {
        "id": "rU3qCB4Xuz6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59518420-71fc-420a-daf6-2d0a2d121021"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'COVID-19 pandemic', 'rank': 0.0377},\n",
              " {'title': 'Pandemic', 'rank': 0.0337},\n",
              " {'title': 'Crimson Contagion', 'rank': 0.0272},\n",
              " {'title': 'Pandemic Severity Assessment Framework', 'rank': 0.0232},\n",
              " {'title': 'Disease X', 'rank': 0.0225},\n",
              " {'title': 'Science diplomacy and pandemics', 'rank': 0.0168}]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search(\"ebola\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQtZqLmbjcQl",
        "outputId": "b20456e7-5f0a-4216-a1d8-bf9b3adcb8dc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Plague of Cyprian', 'rank': 0.0925},\n",
              " {'title': 'Science diplomacy and pandemics', 'rank': 0.0503}]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search(\"dengue\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6pLse-_djehy",
        "outputId": "c28ad064-891d-438c-e3f8-c2f6b4f92a3f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No Relevant documents Found'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save this new list of dictionaries as a JSON file.\n"
      ],
      "metadata": {
        "id": "U1SlwbR9IhyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('search_results.json', 'w') as fp:\n",
        "    json.dump(data, fp)\n"
      ],
      "metadata": {
        "id": "nMI0ylvRyqto"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}