{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8Y2pou8vP3UKq6hRetCcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnasaiv/personal_projects/blob/main/searchEngine/1.2%20TF-IDF%20Search%20Using%20Cosine%20Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dependencies, or libraries required to complete the step."
      ],
      "metadata": {
        "id": "oR78xyPm1i7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qHgrp_ZJ1LEO"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "import spacy \n",
        "import math\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open the JSON file containing the provided dataset\n",
        "which is a list of dictionaries containing a title, URL, and summary from a set of Wikipedia articles.\n"
      ],
      "metadata": {
        "id": "fy9fwFYR1ny5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data_lemmatized.json') as fp:\n",
        "  file_contents = fp.read()\n",
        "\n",
        "data = json.loads(file_contents)\n",
        "# data"
      ],
      "metadata": {
        "id": "4xCwyEXX1sAw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a vocabulary for your dataset, for example, a set of tokens occurring in all your texts"
      ],
      "metadata": {
        "id": "t5WbeOSxISIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_docs = len(data)\n",
        "tokenized_docs = [i['tokenized_text'] for i in data]\n",
        "\n",
        "vocab = list(chain.from_iterable(tokenized_docs))\n",
        "corpus = Counter(vocab)\n",
        "\n",
        "with open('data_corpus.json', 'w') as fp:\n",
        "    json.dump(data, fp)"
      ],
      "metadata": {
        "id": "v4smxlH1IUT-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Develop a Tf-Idf-builder function that accepts a vocabulary and a text string and returns the document’s Tf-Idf vector.\n"
      ],
      "metadata": {
        "id": "nzHT-IaPIU4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TfIdf(all_tokenized_docs, tokenized_doc):\n",
        "    #### Create a zero vector from corpus with sorted keys. \n",
        "    #### This is to ensure the value for each element of the vector to represent the same word in each document’s vector\n",
        "    vec = OrderedDict(( (i,0) for i in sorted(corpus.keys()) ))\n",
        "    # print(len(vec))\n",
        "    token_frequencies = Counter(tokenized_doc)\n",
        "\n",
        "    for token in tokenized_doc:\n",
        "\n",
        "        if token in vec:\n",
        "            ######## TF\n",
        "            tf = token_frequencies[token] / len(token_frequencies)\n",
        "            ######## IDF\n",
        "            docs_with_key = sum([ 1 if token in doc else 0 for doc in all_tokenized_docs]) \n",
        "            if docs_with_key:\n",
        "                idf = len(all_tokenized_docs)/ sum([ 1 if token in doc else 0 for doc in all_tokenized_docs]) \n",
        "            else:\n",
        "                idf = 0\n",
        "            ######## TF-IDF\n",
        "            vec[token] = tf * idf\n",
        "    \n",
        "    return list(vec.values())"
      ],
      "metadata": {
        "id": "Ja7z5JdNIXSV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Run the Tf-Idf-builder function on all text documents in the dataset.\n",
        "\n",
        "### 2. Update the original list of dictionaries by adding tf_idf as a new field to each document dictionary called tf_idf"
      ],
      "metadata": {
        "id": "gEyn5inSIY1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in data:\n",
        "    doc['tf_idf'] = TfIdf(tokenized_docs, doc['tokenized_text'])\n",
        "\n",
        "with open('data_tfidf.json', 'w') as fp:\n",
        "    json.dump(data, fp)"
      ],
      "metadata": {
        "id": "7bZ8pRjZIbDj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a search function to compute cosine similarities between the document Tf-Idf vectors and the query Tf-Idf vector."
      ],
      "metadata": {
        "id": "3gLCG-V7IfO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a,b):\n",
        "    dot_product = 0\n",
        "    for i in range(len(a)):\n",
        "        dot_product += a[i] * b[i]\n",
        "\n",
        "    mag_1 = math.sqrt(sum([x**2 for x in a]))\n",
        "    mag_2 = math.sqrt(sum([x**2 for x in b]))\n",
        "\n",
        "    return dot_product / (mag_1 * mag_2)\n",
        "\n",
        "def tokenizer(input_string):\n",
        "    input_string = input_string.lower()\n",
        "    doc = nlp(input_string.replace(\"\\n\", \"\"))\n",
        "    tokens = [token.lemma_ for token in doc if \n",
        "              not (\n",
        "                    token.is_stop or                    # Remove Stop Words\n",
        "                    token.is_punct or                   # Remove punctutaion\n",
        "                    token.pos_ in ('SYM', 'X') or       # Remove symbols & unclassified POS\n",
        "                    token.dep_ == \"\" or                 # Remove unclassified dependencies\n",
        "                    token.like_num or                   # Remove numeric or numeric like tokens\n",
        "                    token.text in '_\\n '                # Remove misc characters\n",
        "                   )\n",
        "              ]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def search(query, data): # the input 'data' is the master dictionary having title, text, tokens, url & tfidf score\n",
        "    tokens = tokenizer(query)\n",
        "    query_TfIdf =  TfIdf([i['tokenized_text'] for i in data], tokens)\n",
        "\n",
        "    search_relevance_ranks = [{doc['title']:cosine_similarity(query_TfIdf, doc['tf_idf'])}  for doc in data]\n",
        "\n",
        "    return sorted(search_relevance_ranks, key= lambda x:list(x.values())[0], reverse=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0j-0gfZCIhYk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search(\"Plague\", data)"
      ],
      "metadata": {
        "id": "rU3qCB4Xuz6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7139c7a2-1248-4225-89cb-aed40cca2ec5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Antonine Plague': 0.2840070552328603},\n",
              " {'Plague of Cyprian': 0.24676801929099304},\n",
              " {'Pandemic': 0.06740938038462299},\n",
              " {'Epidemiology of HIV/AIDS': 0.0},\n",
              " {'Basic reproduction number': 0.0},\n",
              " {'Bills of mortality': 0.0},\n",
              " {'Cholera': 0.0},\n",
              " {'COVID-19 pandemic': 0.0},\n",
              " {'Crimson Contagion': 0.0},\n",
              " {'Disease X': 0.0},\n",
              " {'Event 201': 0.0},\n",
              " {'HIV/AIDS': 0.0},\n",
              " {'HIV/AIDS in Yunnan': 0.0},\n",
              " {'Pandemic prevention': 0.0},\n",
              " {'Pandemic Severity Assessment Framework': 0.0},\n",
              " {'Pandemic severity index': 0.0},\n",
              " {'PREDICT (USAID)': 0.0},\n",
              " {'1929–1930 psittacosis pandemic': 0.0},\n",
              " {'Science diplomacy and pandemics': 0.0},\n",
              " {'Spanish flu': 0.0},\n",
              " {'Superspreader': 0.0},\n",
              " {'Swine influenza': 0.0},\n",
              " {'Targeted immunization strategies': 0.0},\n",
              " {'Unified Victim Identification System': 0.0},\n",
              " {'Viral load': 0.0},\n",
              " {'Virus': 0.0}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save this new list of dictionaries as a JSON file.\n"
      ],
      "metadata": {
        "id": "U1SlwbR9IhyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('search_results.json', 'w') as fp:\n",
        "    json.dump(data, fp)"
      ],
      "metadata": {
        "id": "nMI0ylvRyqto"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}